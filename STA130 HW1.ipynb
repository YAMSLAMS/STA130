{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e6a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Data Counts:\n",
      "Pregnant               0\n",
      "Glucose                5\n",
      "Diastolic_BP          35\n",
      "Skin_Fold            227\n",
      "Serum_Insulin        374\n",
      "BMI                   11\n",
      "Diabetes_Pedigree      0\n",
      "Age                    0\n",
      "Class                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/YBI-Foundation/Dataset/main/Diabetes%20Missing%20Data.csv\")\n",
    "\n",
    "# Get the missing data counts\n",
    "missing_data_counts = data.isnull().sum()\n",
    "print(\"\\nMissing Data Counts:\")\n",
    "print(missing_data_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d02d0096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pregnant', 'Glucose', 'Diastolic_BP', 'Skin_Fold', 'Serum_Insulin',\n",
      "       'BMI', 'Diabetes_Pedigree', 'Age', 'Class'],\n",
      "      dtype='object')\n",
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "print(data.shape)  # This will print (number_of_rows, number_of_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728426c2",
   "metadata": {},
   "source": [
    "2.2 A variable is a changeable/flexible element that represents some part of a dataset. An observation can be considered to be gathered information from tests or other mediums which contain data that would be collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7f06f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         survived      pclass       sibsp       parch        fare\n",
      "count  891.000000  891.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000    8.000000    6.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already loaded your dataset into a DataFrame named df\n",
    "# For demonstration, let's create a sample DataFrame\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Get statistical summary of the numerical columns\n",
    "summary = df.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61e3d11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 15)\n",
      "Index([  0,   2,   4,   5,   7,   8,   9,  12,  13,  14,\n",
      "       ...\n",
      "       878, 880, 881, 882, 883, 884, 885, 886, 888, 890],\n",
      "      dtype='int64', length=690)\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         0\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      0\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "         survived      pclass       sibsp       parch        fare\n",
      "count  889.000000  889.000000  889.000000  889.000000  889.000000\n",
      "mean     0.382452    2.311586    0.524184    0.382452   32.096681\n",
      "std      0.486260    0.834700    1.103705    0.806761   49.697504\n",
      "min      0.000000    1.000000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000    0.000000    0.000000    7.895800\n",
      "50%      0.000000    3.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000    8.000000    6.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.shape)\n",
    "\n",
    "del df['age']\n",
    "#del df['deck']\n",
    "\n",
    "missing_data_indices = df[df.isna().any(axis=1)].index\n",
    "print(missing_data_indices)\n",
    "\n",
    "df.drop(index=[61, 829], inplace=True)\n",
    "\n",
    "print(df.isna().sum())\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9aa9e",
   "metadata": {},
   "source": [
    "In the code above, when finding the dimensions of the dataset with df.shape, we find that there are 891 rows and 15 columns. However, when using the df.describe function to try and summarize the code, we get the class \"age\" to only have  714 rows and see only  6 columns out of 15. Firstly, ages row count is less tthan tthe shape of the dataset becasue itt has missing values. We can find with the function df.isna().sum() that age has 177  missing values, which is the exact difference between the number of rows and age row count, further proving how the age row count discrepancy is caused by its lacking values. Secondly, the reason why df.describe shows fewer columns then what df.shape says there are because there are  9 columns in the dataset that are non-numerical. Since df.describe only looks at numerical values,  then these non-numerical columns are not shown when using this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe104e",
   "metadata": {},
   "source": [
    "An attribute is a variable that only states and stores information about an object to which it is also attached. A key characteristic of attributes is that they are called upon without the use of parenthesis. This contrasts an attribute as a method or \"function\" using parenthesis. Furthermore, methods don't state or keep data from an object but instead perform computations or operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe16039",
   "metadata": {},
   "source": [
    "Count: The number of non-missing values in each column\n",
    "Mean: The average of all the values in a column\n",
    "Std: The measure of the deviation over a data set, based on the mean\n",
    "Min: The minimum value in each column\n",
    "Max: The maximum value in each column\n",
    "25%: The twenty-fifth percentile of the data. Also known as the first quartile, denoted Q1\n",
    "50%: The fiftieth percentile of the data. Also known as the median\n",
    "75%: The seventy-fifth percentile of the data. Also known as the third quartile, denoted Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ce791",
   "metadata": {},
   "source": [
    "A scenario where it would be beneficial to use df.dropna() over del df['col'] is when a dataset has more missing data in rows than columns. For instance, a dataset where the rows contain the ages of people and the columns contain headings of things only 18-year-olds can do, then the rows of people aged 1-17 will be empty. If we were to remove the rows containing missing values, then all rows would be removed as every column contains a row(s) of ages that are not 18+, thus being empty. However, by removing rows containing nothing, then only 17 completely empty rows will be removed, and no data will be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44973fc9",
   "metadata": {},
   "source": [
    "A scenario where it would be beneficial to use del df['col'] over df.dropna() is when a dataset has more missing data in a column than rows. For instance, a dataset that would benefit from doing just this is a medical dataset where the columns are comprised of different people, and the rows contain various ailments. If the majority of columns are empty (people don't have certain aliments), then it would be best to remove those columns over the rows containing the missing data because a single person will remove many missing data spots, whereas a row will remove many filled data spots, so to increase data retention df['col'] should be used over df.dropna()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9c169",
   "metadata": {},
   "source": [
    "It could be important to do del df['col'] before df.dropna() when both are going to be used because it decreases the percentage of data lost. Imagine a data set with ten rows and columns with ten missing data points. All but one missing data point are in one column, but the other ones are dispersed in different columns and rows. There are two cases to look at in order to conclude if, in this scenario, we should use del df['col'] first.\n",
    "\n",
    "Case 1: We use df.dropna() then del df['col']\n",
    "\n",
    "By using df.dropna() first, since all rows contain 1 missing  data point then by removing a row, 9% of real data  will be llostt.\n",
    "\n",
    "Case 1: We use del df['col'] then df.dropna()\n",
    "By using del df['col'] first, since 90% of missing data is in one column, tthen by removing thatt column youd only be losing 1% of real data, tthen to remove the final missing data youd have to lose a row (9% of real data). \n",
    "\n",
    "Therefore, by using df.dropna() first you can get rid of 100% of missing data points at the cost of 10% of real data, whereas if you choose the former, you will remove 9% of real data for only 1% of the missing data. So in this case, it is a no-brainer to use df.dropna() first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36747e06",
   "metadata": {},
   "source": [
    "For the data set I used in question 4, for me to remove all missing data, I would remove the columns labelled \"age,\" and \"deck\" with del df['col'] then I would remove the rows labelled \"embark_town\" and \"embark\" using df. drop (). I would remove the missing data to increase data retention. I am using df.isna().sum() helps me see that the majority of missing data is in 2 columns, and using df[df.isna().any(axis=1)].index, I can find which rows have the remaining missing data. Of the 869 missing data, 865 are in 2 columns, and 4 are in two other rows. This results in 903 real data being lost. If I we're to remove the rows and columns containing missing data first, I'd remove over 10000 real data to remove the missing data, so, of course, I'd choose the first method to remove all the missing data as it contains more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5209f5ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min   25%   50%   75%   max\n",
      "Sex                                                              \n",
      "female  261.0  27.915709  14.110146  0.75  18.0  27.0  37.0  63.0\n",
      "male    453.0  30.726645  14.678201  0.42  21.0  29.0  39.0  80.0\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "\n",
    "# Example: Group by 'Sex' and describe 'Age'\n",
    "result = df.groupby(\"Sex\")[\"Age\"].describe()\n",
    "print(result)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33baac54",
   "metadata": {},
   "source": [
    "df.groupby(\"col1\")[\"col2\"].describe() does a few things. (\"col1\") takes the column listed and makes several classes or groups with the number of unique elements it has. [\"col2\"] takes the column listed and puts the column data into one of the classes created by (\"col1\"), depending on each data point corresponding to the row value in (\"col1\"). So, using the Titanic dataset (\"Age\") creates two classes labelled \"female\" or \"male\" as those are the unique values in \"Sex.\" So [\"Age\"] will put its data into either class depending on each of its data points corresponding to the \"Sex\" data point in that row. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbedef",
   "metadata": {},
   "source": [
    "The values in count differ between df.describe() and df.group by (\"Sex\")[\"Age\"].describe() because using df. group by (\"col1\")[\"col2\"].describe() gives a count within subgroups, whereas df.describe() takes into account all non-null data for the whole data set not just certain groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7bde1b",
   "metadata": {},
   "source": [
    "A chatbot is much more useful than Google Search for my problems. This is because I don't understand Python very well, and when I search for my errors on Google, I get results that do answer my question, but since I don't understand the terminology used, I get lost or intimidated and never actually understand what is wrong and how to fix it. Contrastly, a chatbot is much more effective for a person like me as I can converse with it in person, and it will output answers that are comprehensive to someone of my calibre of coding knowledge with answers straight to the point. I'll almost always go to a chatbot for help before googling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9c4b1",
   "metadata": {},
   "source": [
    "Kinda. I read through some of the course wiki-textbook, but mainly relied on a chatbot to help me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1cb23",
   "metadata": {},
   "source": [
    "Chatbot 1 Summary of Interaction:\n",
    "In this session, I sought assistance on various data analysis concepts and methods related to a dataset. The conversation covered the following topics:\n",
    "\n",
    "Loading and Inspecting a Dataset:\n",
    "\n",
    "I was provided with steps to load a dataset from a URL using Python’s pandas library and shown how to inspect its structure, including viewing the columns (df.columns), getting summary statistics (df.describe()), and checking the shape of the data (df.shape).\n",
    "Definitions of Observations and Variables:\n",
    "\n",
    "I learned the definitions of key terms:\n",
    "Observations: The individual entries or records in a dataset (e.g., each character in an Animal Crossing dataset).\n",
    "Variables: The attributes or properties of each observation (e.g., Name, Species, Birthday).\n",
    "Using df.describe() and df['column'].value_counts() Methods:\n",
    "\n",
    "I was given examples of using these methods:\n",
    "df.describe(): Provides a summary of key statistics (count, mean, standard deviation, min, max, and percentiles) for numerical columns.\n",
    "df['column'].value_counts(): Counts the occurrences of each unique value in a column, typically used for categorical or discrete numerical data.\n",
    "Difference Between Attributes and Methods:\n",
    "\n",
    "I was explained the difference between:\n",
    "Attributes: Variables that belong to an object (e.g., df.shape), accessed without parentheses.\n",
    "Methods: Functions that perform actions or computations (e.g., df.describe()), requiring parentheses.\n",
    "Explanation of Summary Statistics from df.describe():\n",
    "\n",
    "A detailed explanation of the following summary statistics was provided: count, mean, standard deviation (std), minimum (min), 25th percentile (25%), median (50%), 75th percentile (75%), and maximum (max). Definitions and examples were shared for each, helping me understand what they represent in data analysis.\n",
    "This conversation helped clarify important data analysis concepts, particularly related to inspecting and summarizing a dataset using pandas in Python. The explanations and examples given will be useful for my assignment on exploring and summarizing data in a structured format.\n",
    "\n",
    "Link: https://chatgpt.com/c/66df97a3-45c8-800c-95f1-a434dc4e941d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f7fd4",
   "metadata": {},
   "source": [
    "Chatbot 2 Summary of Interaction:\n",
    "In this interaction, we addressed a series of common coding errors encountered while working with the Titanic dataset in Python, using the pandas library. Here's a summary of the issues and how they were resolved:\n",
    "\n",
    "NameError: name 'pd' is not defined:\n",
    "\n",
    "Issue: The pandas library wasn’t imported.\n",
    "Solution: The user was instructed to add import pandas as pd to import the pandas library correctly.\n",
    "FileNotFoundError: No such file or directory: 'titanic.csv':\n",
    "\n",
    "Issue: The file 'titanic.csv' could not be found.\n",
    "Solution: The user was advised to check the file name and provide the correct file path, ensuring it matched the location of the dataset.\n",
    "NameError: name 'DF' is not defined:\n",
    "\n",
    "Issue: A variable named DF (uppercase) was used without being defined.\n",
    "Solution: The user was instructed to either define DF properly or use the lowercase df, which is commonly used after loading the dataset.\n",
    "SyntaxError: '(' was never closed:\n",
    "\n",
    "Issue: A missing closing parenthesis caused a syntax error.\n",
    "Solution: The user was guided to ensure all open parentheses were properly closed.\n",
    "AttributeError: 'SeriesGroupBy' object has no attribute 'describle':\n",
    "\n",
    "Issue: A typo in the method .describle() instead of .describe().\n",
    "Solution: The correct method .describe() was provided.\n",
    "KeyError: 'age':\n",
    "\n",
    "Issue: The column name \"age\" was incorrect due to case sensitivity (the correct name was \"Age\").\n",
    "Solution: The user was advised to check column names and correct the case.\n",
    "KeyError: 'sex':\n",
    "\n",
    "Issue: The column name \"sex\" was not found.\n",
    "Solution: The user was instructed to use the correct column name \"Sex\" (case-sensitive) and check for any spaces.\n",
    "NameError: name 'Age' is not defined:\n",
    "\n",
    "Issue: The column name \"Age\" was treated as a variable instead of a string.\n",
    "Solution: The user was advised to enclose column names in quotes (e.g., \"Age\").\n",
    "NameError: name 'Sex' is not defined:\n",
    "\n",
    "Issue: The column name \"Sex\" was treated as a variable instead of a string.\n",
    "Solution: The user was advised to enclose \"Sex\" in quotes to resolve the error.\n",
    "Overall Learnings:\n",
    "Case sensitivity matters when referencing column names in pandas (\"Age\" vs. \"age\").\n",
    "Syntax issues like unclosed parentheses or misspelled methods can cause errors.\n",
    "File paths need to be correct and relative to the working directory.\n",
    "Always ensure column names are enclosed in quotes when referencing them in pandas.\n",
    "By addressing each error systematically, we ensured the user’s code would run successfully with the Titanic dataset.\n",
    "\n",
    "Link: https://chatgpt.com/c/66e235b8-a478-800c-b8aa-b49c80a22b28"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
